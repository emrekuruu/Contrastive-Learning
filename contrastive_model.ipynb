{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearning(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, projection_dim, dropout_rate=0.15):\n",
    "        super(ContrastiveLearning, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, 1028),\n",
    "                nn.BatchNorm1d(1028),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(1028, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(512, embedding_dim),\n",
    "            )\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(256, projection_dim),\n",
    "            )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        projection = self.projector(embedding)\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NTXentLoss(temperature=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(\"data/synthetic_dataset.xlsx\", index_col=0)\n",
    "cluster_labels = pd.read_excel(\"data/clusters.xlsx\", index_col=0)\n",
    "cluster_labels.index = data.index\n",
    "cluster_labels = cluster_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allspice</th>\n",
       "      <th>almond</th>\n",
       "      <th>amaretto</th>\n",
       "      <th>anise</th>\n",
       "      <th>apple</th>\n",
       "      <th>applesauce</th>\n",
       "      <th>apricot</th>\n",
       "      <th>artichoke</th>\n",
       "      <th>arugula</th>\n",
       "      <th>asparagus</th>\n",
       "      <th>...</th>\n",
       "      <th>watercress</th>\n",
       "      <th>watermelon</th>\n",
       "      <th>wheat</th>\n",
       "      <th>whip</th>\n",
       "      <th>whiskey</th>\n",
       "      <th>wine</th>\n",
       "      <th>wrapper</th>\n",
       "      <th>yeast</th>\n",
       "      <th>yoghurt</th>\n",
       "      <th>yogurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71616</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71617</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71618</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69265 rows Ã— 343 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       allspice  almond  amaretto  anise  apple  applesauce  apricot  \\\n",
       "0             0       0         0      0      0           0        0   \n",
       "1             0       0         0      0      0           0        0   \n",
       "2             0       0         0      0      0           0        0   \n",
       "3             0       0         0      0      0           0        0   \n",
       "4             0       0         0      0      0           0        0   \n",
       "...         ...     ...       ...    ...    ...         ...      ...   \n",
       "71616         0       0         0      0      0           0        0   \n",
       "71617         0       1         0      0      0           0        0   \n",
       "71618         0       0         0      0      0           0        0   \n",
       "71619         0       0         0      0      0           0        0   \n",
       "71620         0       0         0      0      0           0        0   \n",
       "\n",
       "       artichoke  arugula  asparagus  ...  watercress  watermelon  wheat  \\\n",
       "0              0        0          0  ...           0           0      0   \n",
       "1              0        0          0  ...           0           0      0   \n",
       "2              0        0          0  ...           0           0      0   \n",
       "3              0        0          0  ...           0           0      0   \n",
       "4              0        0          0  ...           0           0      0   \n",
       "...          ...      ...        ...  ...         ...         ...    ...   \n",
       "71616          0        0          0  ...           0           0      0   \n",
       "71617          0        0          0  ...           0           0      0   \n",
       "71618          0        0          0  ...           0           0      0   \n",
       "71619          0        0          0  ...           0           0      0   \n",
       "71620          0        0          0  ...           0           0      0   \n",
       "\n",
       "       whip  whiskey  wine  wrapper  yeast  yoghurt  yogurt  \n",
       "0         0        0     0        0      0        0       0  \n",
       "1         0        0     0        0      0        0       0  \n",
       "2         0        0     0        0      0        0       0  \n",
       "3         0        0     0        0      0        0       0  \n",
       "4         0        0     0        0      0        0       0  \n",
       "...     ...      ...   ...      ...    ...      ...     ...  \n",
       "71616     0        0     0        0      0        0       0  \n",
       "71617     0        0     0        0      0        0       0  \n",
       "71618     0        0     0        0      0        0       0  \n",
       "71619     0        0     0        0      0        0       0  \n",
       "71620     0        0     0        0      0        0       0  \n",
       "\n",
       "[69265 rows x 343 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "71616    3\n",
       "71617    0\n",
       "71618    4\n",
       "71619    1\n",
       "71620    4\n",
       "Name: 4, Length: 69265, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_dim = data.shape[1]\n",
    "embedding_dim = 32\n",
    "projection_dim = 8\n",
    "\n",
    "model = ContrastiveLearning(input_dim, embedding_dim, projection_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterContrastiveDataset(Dataset):\n",
    "    def __init__(self, data, cluster_labels):\n",
    "        self.data = data\n",
    "        self.cluster_labels = cluster_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        label = self.cluster_labels.iloc[idx]\n",
    "        positive_indices = [i for i, same_label in enumerate(self.cluster_labels) if same_label == label and i != idx]\n",
    "        positive_idx = random.choice(positive_indices)\n",
    "        positive_item = self.data.iloc[positive_idx]\n",
    "\n",
    "        item_tensor = torch.tensor(item, dtype=torch.float32)\n",
    "        positive_item_tensor = torch.tensor(positive_item, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return item_tensor, positive_item_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClusterContrastiveDataset(data=data, cluster_labels=cluster_labels)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, log_interval): \n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data_i, data_j) in enumerate(dataloader):\n",
    "\n",
    "            data_i, data_j = data_i.float().to(device), data_j.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "\n",
    "            projections_i = model(data_i)\n",
    "            projections_j = model(data_j)\n",
    "\n",
    "            # Concatenate the projections: \n",
    "            # The positive pairs are adjacent to each other, and all others are considered negatives.\n",
    "            projections = torch.cat([projections_i, projections_j], dim=0)\n",
    "            \n",
    "            batch_size = projections_i.size(0)\n",
    "            labels = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "            labels = torch.cat((labels, labels), dim=0)  # Duplicate labels for both halves of concatenated data\n",
    "\n",
    "            # Calculate the contrastive loss\n",
    "            loss = criterion(projections, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Convert the Pandas series to a tensor and add an extra batch dimension\n",
    "single_sample = torch.tensor(dataset.data.iloc[100].values).float().unsqueeze(0)\n",
    "\n",
    "model.encoder(single_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finch import FINCH\n",
    "\n",
    "embeddings = model.encoder(torch.tensor(dataset.data.values).float()).detach()\n",
    "\n",
    "if embeddings.is_cuda:\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "embeddings_np = embeddings.numpy()\n",
    "\n",
    "c, num_clust, req_c = FINCH(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters =  pd.DataFrame(c)[4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari_score = adjusted_rand_score(cluster_labels.values, new_clusters)\n",
    "nmi_score = normalized_mutual_info_score( cluster_labels.values, new_clusters)\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari_score)\n",
    "print(\"Normalized Mutual Information:\", nmi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embeddings, index = cluster_labels.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'embeddings' is a numpy array of your data embeddings\n",
    "# And 'cluster_labels' is an array of cluster labels corresponding to each point in 'embeddings'\n",
    "embeddings_2d = TSNE(n_components=2, random_state=0).fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=new_clusters, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar()  # To show the color scale\n",
    "plt.title('t-SNE plot of the embeddings colored by cluster label')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_excel(\"data/synthetic_dataset.xlsx\", index_col=0)\n",
    "embeddings_2d = TSNE(n_components=2, random_state=0).fit_transform(data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar()  # To show the color scale\n",
    "plt.title('t-SNE plot of the actual data')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_excel(\"data/clustered_data.xlsx\", index_col=0)\n",
    "recipes.drop(\"cluster_labels\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_excel(\"data/recipe_logs.xlsx\", index_col=0)\n",
    "test_set.drop(\"id\", axis=1, inplace=True)\n",
    "users_feedback = test_set\n",
    "labels = users_feedback.is_accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = recipes.loc[users_feedback.recipe_id.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(recipes, labels, test_size=0.20, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR = LogisticRegression(penalty='l2', C=0.1, n_jobs=-1, max_iter=1000)\n",
    "contastive_LR = LogisticRegression(penalty='l2', C=0.1, n_jobs=-1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR.fit(X_train,y_train)\n",
    "embeddings = model.encoder(torch.tensor(X_train.values).float()).detach()\n",
    "embedding_test = model.encoder(torch.tensor(X_test.values).float()).detach()\n",
    "contastive_LR.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contastive_LR.score(embeddings,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contastive_LR.predict(embeddings).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contastive_LR.score(embedding_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
